{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0IVOKbtJnrFfYEBjuJHll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LilySaya/MachineLearning/blob/main/LogisticRegressionSimple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0LQShfumHqa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "class MyLogisticRegression():\n",
        "  def __init__ (self, lr = 0.001, n_iters = 1000):\n",
        "    self.lr = lr\n",
        "    self.n_iters = n_iters\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "\n",
        "\n",
        "  def fit(self, x, y):\n",
        "    n_samples, n_features = x.shape\n",
        "    self.weights = np.zeros(n_features)\n",
        "    self.bias = 0\n",
        "\n",
        "    for _ in range(self.n_iters):\n",
        "        linear_pred = np.dot(x, self.weights) + self.bias\n",
        "        predictions = sigmoid(linear_pred)\n",
        "\n",
        "        dw = (1/n_samples) * np.dot(x.T, (predictions - y))\n",
        "        db = (1/n_samples) * np.sum(predictions-y)\n",
        "\n",
        "        self.weights = self.weights - self.lr*dw\n",
        "        self.bias = self.bias - self.lr*db\n",
        "    \n",
        "    print(np.shape(predictions))\n",
        "    print(self.weights, self.bias)\n",
        "    print(predictions)\n",
        "\n",
        "  def predict(self, x):\n",
        "      linear_pred = np.dot(x, self.weights) + self.bias\n",
        "      y_pred = sigmoid(linear_pred)\n",
        "      class_pred = [0 if y<=0.5 else 1 for y in y_pred]\n",
        "      return class_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "df = pd.DataFrame(iris.data, columns = iris.feature_names).assign(target = iris.target)\n",
        "lookup_flower_name = df.target.unique()\n",
        "\n",
        "x = df[['petal length (cm)','petal width (cm)']]\n",
        "y = df['target']\n",
        "\n",
        "x_train, x_test, y_train, y_test =  train_test_split(x,y, random_state = 0, test_size = 0.1)\n",
        "\n",
        "clf = MyLogisticRegression()\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "def accuracy (y_pred, y_test):\n",
        "  return np.sum(y_pred == y_test)/len(y_test)\n",
        "\n",
        "acc = accuracy(y_pred, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNEG4AAcp2Lo",
        "outputId": "f933002c-330e-4735-f2a1-881b46901400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(135,)\n",
            "[1.64760047 0.678695  ] 0.10064211090484033\n",
            "[0.92223455 0.99980172 0.99971352 0.93142064 0.92497246 0.99992687\n",
            " 0.99980172 0.96656052 0.92696052 0.99990126 0.86789049 0.97067706\n",
            " 0.99968441 0.99799708 0.95283394 0.99996881 0.99980172 0.93735217\n",
            " 0.99991624 0.99997983 0.99943045 0.95699891 0.99993099 0.99967513\n",
            " 0.99944672 0.99997455 0.91500148 0.99995707 0.95856785 0.93735217\n",
            " 0.99855814 0.99996881 0.99998604 0.99991293 0.99998096 0.99930205\n",
            " 0.99999172 0.99986668 0.99917726 0.99991624 0.99992614 0.99982686\n",
            " 0.99993361 0.99984732 0.99995537 0.99981471 0.94635092 0.99994369\n",
            " 0.99962796 0.99900167 0.99948297 0.99982001 0.9999863  0.93735217\n",
            " 0.87860066 0.99995949 0.99960187 0.95412036 0.94485925 0.99989735\n",
            " 0.93735217 0.99993974 0.99693639 0.93142064 0.99980172 0.99999277\n",
            " 0.99962796 0.92223455 0.99998682 0.99998401 0.99998549 0.99997841\n",
            " 0.94635092 0.94635092 0.99994738 0.99998169 0.92012894 0.9999738\n",
            " 0.92696052 0.99993797 0.99996999 0.91500148 0.92696052 0.9999782\n",
            " 0.93142064 0.91500148 0.95970449 0.99948297 0.9999885  0.99999584\n",
            " 0.92696052 0.93735217 0.92696052 0.99977292 0.99974985 0.90129283\n",
            " 0.96224655 0.99968441 0.94485925 0.99999778 0.99799708 0.99999381\n",
            " 0.99974985 0.93325017 0.99990126 0.90129283 0.99999622 0.94121797\n",
            " 0.94635092 0.99998914 0.92696052 0.99998238 0.99956142 0.99934778\n",
            " 0.99855814 0.99996911 0.99996358 0.99989735 0.99992397 0.94635092\n",
            " 0.99896221 0.99998238 0.99994369 0.93735217 0.99980738 0.99956142\n",
            " 0.99990126 0.9997323  0.91500148 0.94485925 0.93325017 0.99997354\n",
            " 0.99946253 0.9999967  0.92696052]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zpkJ9pXMqjrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}